services:
  backend:
    build:
      context: ./apps/backend/
      dockerfile: Dockerfile
    env_file:
      - apps/backend/.env
    ports:
      - 5000:5000
    restart: unless-stopped
    volumes:
      - ./apps/backend/data:/app/data
    healthcheck:
      test:
        - CMD
        - python
        - -c
        - "import socket; socket.create_connection(('localhost', 5000), 3)"
      interval: 30s
      timeout: 10s
      retries: 5
    depends_on:
      - llama

  llama:
    build:
      context: ./apps/llama
      dockerfile: Dockerfile
    cap_add:
      - SYS_RESOURCE
    environment:
      MODEL_URL: "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_0.gguf"
      MODEL_PATH: "/models/tinyllama-1.1b-chat-v1.0.Q4_0.gguf"
      # MODEL_SHA256: ""  # optional integrity check
    ports:
      - 3000:3000/tcp
    volumes:
      - ./apps/llama/models:/models
